"""
YOLO12 ç›®æ ‡æ£€æµ‹è®­ç»ƒè„šæœ¬ - äº‘æœåŠ¡å™¨ç‰ˆæœ¬
é€‚ç”¨äº NVIDIA T4 GPU (16GB)
CPU: 8æ ¸, å†…å­˜: 16GB

ä¸»è¦ä¼˜åŒ–:
- ä½¿ç”¨ CUDA åŠ é€Ÿ
- å¢åŠ  batch size
- å¢åŠ  workers æ•°é‡
- ä¼˜åŒ–è®­ç»ƒé€Ÿåº¦
"""

from ultralytics import YOLO
import torch
import os

def main():
    # ============ é…ç½®å‚æ•° (T4 GPU ä¼˜åŒ–) ============
    
    # æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
    data_yaml = 'datasets/data.yaml'
    
    # æ¨¡å‹é€‰æ‹©
    # é€‰é¡¹: yolo12n.pt (æœ€å¿«), yolo12s.pt (æ¨è), yolo12m.pt (æ›´å¥½æ•ˆæœ)
    model_name = 'yolo12s.pt'  # T4å¯ä»¥ç”¨æ›´å¤§çš„æ¨¡å‹
    
    # è®­ç»ƒå‚æ•° - T4 GPUä¼˜åŒ–
    epochs = 100             # T4é€Ÿåº¦å¿«ï¼Œå¯ä»¥è®­ç»ƒæ›´å¤šè½®
    batch_size = 32          # T4 16GBå¯ä»¥ç”¨32ï¼Œå¦‚æœæ˜¾å­˜ä¸è¶³é™åˆ°24æˆ–16
    img_size = 640           # å›¾åƒå°ºå¯¸
    
    # è®¾å¤‡è®¾ç½®
    device = 0               # ä½¿ç”¨ç¬¬ä¸€å—GPU (cuda:0)
    
    # æ•°æ®åŠ è½½ä¼˜åŒ– - 8æ ¸CPU
    workers = 8              # è®¾ç½®ä¸ºCPUæ ¸å¿ƒæ•°
    
    # è®­ç»ƒç­–ç•¥
    patience = 30            # æ—©åœè€å¿ƒå€¼ï¼Œå¯ä»¥æ›´å¤§å› ä¸ºè®­ç»ƒå¿«
    cache = 'ram'            # å°†æ•°æ®é›†ç¼“å­˜åˆ°å†…å­˜(16GBå¤Ÿç”¨)ï¼ŒåŠ é€Ÿè®­ç»ƒ
    
    # ä¿å­˜è·¯å¾„
    project_name = 'runs/detect'
    experiment_name = 'yolo12s_person_head_t4'  # æ ‡æ³¨æ˜¯T4è®­ç»ƒçš„
    
    # ============ æ£€æŸ¥ç¯å¢ƒ ============
    print("=" * 60)
    print("ğŸš€ YOLO12 è®­ç»ƒé…ç½® (äº‘æœåŠ¡å™¨ - T4 GPU)")
    print("=" * 60)
    print(f"ğŸ“Š æ•°æ®é›†: {data_yaml}")
    print(f"ğŸ¤– æ¨¡å‹: {model_name}")
    print(f"ğŸ’» è®¾å¤‡: CUDA (GPU)")
    print(f"ğŸ”¢ Epochs: {epochs}")
    print(f"ğŸ“¦ Batch Size: {batch_size}")
    print(f"ğŸ“ å›¾åƒå°ºå¯¸: {img_size}")
    print(f"ğŸ‘· Workers: {workers}")
    print(f"ğŸ’¾ ç¼“å­˜: {cache}")
    print("=" * 60)
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    if not torch.cuda.is_available():
        print("\nâš ï¸  è­¦å‘Š: CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆä¼šå¾ˆæ…¢ï¼‰")
        device = 'cpu'
        batch_size = 8
        workers = 4
        cache = False
    else:
        print(f"\nâœ… CUDAå¯ç”¨")
        print(f"   GPUåç§°: {torch.cuda.get_device_name(0)}")
        print(f"   GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_yaml):
        raise FileNotFoundError(f"æ•°æ®é›†é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {data_yaml}")
    
    # ============ åŠ è½½æ¨¡å‹ ============
    print("\nğŸ“¥ åŠ è½½æ¨¡å‹...")
    model = YOLO(model_name)
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
    
    # ============ å¼€å§‹è®­ç»ƒ ============
    print("\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...\n")
    
    try:
        results = model.train(
            data=data_yaml,
            epochs=epochs,
            batch=batch_size,
            imgsz=img_size,
            device=device,
            workers=workers,
            cache=cache,              # ç¼“å­˜æ•°æ®é›†åˆ°å†…å­˜
            
            # ä¼˜åŒ–å‚æ•°
            patience=patience,
            save=True,
            save_period=20,           # æ¯20ä¸ªepochä¿å­˜ä¸€æ¬¡ï¼ˆè®­ç»ƒå¿«äº†å¯ä»¥å°‘ä¿å­˜ï¼‰
            
            # ä¼˜åŒ–å™¨è®¾ç½® (T4å¯ä»¥ç”¨æ›´æ¿€è¿›çš„è®¾ç½®)
            optimizer='AdamW',        # AdamWé€šå¸¸æ¯”SGDæ”¶æ•›å¿«
            lr0=0.001,               # åˆå§‹å­¦ä¹ ç‡
            lrf=0.01,                # æœ€ç»ˆå­¦ä¹ ç‡
            momentum=0.937,
            weight_decay=0.0005,
            warmup_epochs=3.0,        # warmupè½®æ•°
            warmup_momentum=0.8,
            warmup_bias_lr=0.1,
            
            # æ•°æ®å¢å¼º (å¯ä»¥æ›´æ¿€è¿›)
            hsv_h=0.015,
            hsv_s=0.7,
            hsv_v=0.4,
            degrees=0.0,
            translate=0.1,
            scale=0.5,
            shear=0.0,
            perspective=0.0,
            flipud=0.0,
            fliplr=0.5,
            mosaic=1.0,
            mixup=0.15,              # T4å¯ä»¥ç”¨mixupå¢å¼º
            copy_paste=0.0,
            
            # ä¿å­˜è®¾ç½®
            project=project_name,
            name=experiment_name,
            exist_ok=True,
            
            # å…¶ä»–
            pretrained=True,
            verbose=True,
            seed=42,
            deterministic=False,      # T4ä¸Šå¯ä»¥å…³é—­ç¡®å®šæ€§ä»¥è·å¾—æ›´å¿«é€Ÿåº¦
            single_cls=False,         # å¤šç±»åˆ«æ£€æµ‹
            rect=False,               # çŸ©å½¢è®­ç»ƒï¼ˆå¯é€‰ï¼‰
            cos_lr=True,              # ä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡è¡°å‡
            close_mosaic=10,          # æœ€å10ä¸ªepochå…³é—­mosaic
            
            # éªŒè¯è®¾ç½®
            val=True,
            plots=True,
            
            # GPUä¼˜åŒ–
            amp=True,                 # è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ŒåŠ é€Ÿè®­ç»ƒå¹¶èŠ‚çœæ˜¾å­˜
        )
        
        print("\n" + "=" * 60)
        print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        print("=" * 60)
        
        # æ˜¾ç¤ºæœ€ä½³æ¨¡å‹è·¯å¾„
        best_model_path = f"{project_name}/{experiment_name}/weights/best.pt"
        last_model_path = f"{project_name}/{experiment_name}/weights/last.pt"
        
        print(f"\nğŸ“ è®­ç»ƒç»“æœä¿å­˜åœ¨: {project_name}/{experiment_name}/")
        print(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model_path}")
        print(f"ğŸ“ æœ€åæ¨¡å‹: {last_model_path}")
        
        # ============ éªŒè¯æ¨¡å‹ ============
        print("\nğŸ” åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹...")
        
        best_model = YOLO(best_model_path)
        metrics = best_model.val(data=data_yaml, device=device)
        
        print("\nğŸ“Š éªŒè¯é›†æ€§èƒ½æŒ‡æ ‡:")
        print(f"  mAP50: {metrics.box.map50:.4f}")
        print(f"  mAP50-95: {metrics.box.map:.4f}")
        print(f"  Precision: {metrics.box.mp:.4f}")
        print(f"  Recall: {metrics.box.mr:.4f}")
        
        # ============ åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° ============
        print("\nğŸ” åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹...")
        test_metrics = best_model.val(data=data_yaml, split='test', device=device)
        
        print("\nğŸ“Š æµ‹è¯•é›†æ€§èƒ½æŒ‡æ ‡:")
        print(f"  mAP50: {test_metrics.box.map50:.4f}")
        print(f"  mAP50-95: {test_metrics.box.map:.4f}")
        print(f"  Precision: {test_metrics.box.mp:.4f}")
        print(f"  Recall: {test_metrics.box.mr:.4f}")
        
        print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
        
        # æ˜¾ç¤ºè®­ç»ƒæ—¶é—´ç»Ÿè®¡
        print("\nâ±ï¸  è®­ç»ƒç»Ÿè®¡:")
        print(f"  é¢„è®¡è®­ç»ƒæ—¶é—´: ~{epochs * 0.5:.0f}-{epochs * 1:.0f}åˆ†é’Ÿ (T4)")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        print(f"ğŸ’¾ éƒ¨åˆ†è®­ç»ƒç»“æœå·²ä¿å­˜åœ¨: {project_name}/{experiment_name}/")
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == '__main__':
    # æ‰“å°ç³»ç»Ÿä¿¡æ¯
    print("\nğŸ–¥ï¸  ç³»ç»Ÿä¿¡æ¯:")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰GPU: {torch.cuda.current_device()}")
        print(f"GPUåç§°: {torch.cuda.get_device_name(0)}")
    
    # å¼€å§‹è®­ç»ƒ
    main()
